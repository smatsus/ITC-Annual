\subsection{研究報告（松島）}
本節では松島研究室の研究活動について報告する。
2019年度および2020年度に、弊研究室では解釈可能な機械学習手法の効率的な計算手法についての研究を推進してきた。

大量のデータから複雑な関数を推定することにより画像データや言語データを予測したり生成したりするなど、表層的に駆使することは可能になってきた。
しかしながら我々の画像や言語に対する理解が進んだわけではない。現代社会には様々なデータがあり、データを上記の意味で駆使するだけでなく、データの隠れた法則性や生成原理などの理解に結び付く属性間の関係を抽出することが求められる。

機械学習は予測や分類など表層的なデータの駆使の方法論であるだけでなく、
データの関係を明らかにして人間の理解を助けるための方法論でもあり、
特にデータ保持者の目線に立った機械学習の手法の研究を行ってきた。
成果は主に以下の３分野に大別される。
\begin{itemize}
    \item 一般化加法モデルに関する研究
    \item 組合せ線形モデルに関する研究
    \item 部分空間クラスタリングに関する研究
\end{itemize}

一般化加法モデルと組み合わせ線形モデルは
属性間の線形な関係を越えて、非線形な関係を抽出するための枠組みである。
部分空間クラスタリングはデータ集合が持つ単純な線形関係を超えて、
データのクラスタリングを行ってそれぞれのクラスタが持つ線形関係を抽出する枠組みである。


\subsubsection{一般化加法モデルに関する研究}
教師あり学習の文脈において、線形モデルの学習とは以下のようにあらわされるデータの属性間の線形な関係を抽出する枠組みととらえることができる：
\begin{align*}
    y = \sum_{j=1}^d w_j x_j
\end{align*}
データの属性$y$は通常予測したい変数であり、
予測のために用いられるデータの他の属性が$x_j$であらわされる（$j=1,\ldots,d$）。与えられたデータ集合を用いて各$w_j$は実数全体から推定される。
一般化加法モデル\cite{F}は線形モデルと同様に以下のような関係を抽出する枠組みである：
\begin{align*}
    y = \sum_{j=1}^d f_j (x_j)
\end{align*}
このとき、与えられたデータ集合を用いて各$f_j$は（十分広い）関数クラス$F$から推定される。
このような$f_j$を推定できれば、例えば年齢と収入の非線形な関係などがデータから学習できると考えられる。

最も単純な手法は$F$を推定の簡単さのために狭めの関数クラスに制限することである。一般に与えられた基底関数集合$\left\{\varphi_k(\cdot):\mathbb{R} \to \mathbb{R} \right\}_{k=1,\ldots,K}$に対し
\begin{align*}
  F=\left\{  \sum_{k=1}^{d’} \varphi_k(x_j) \right\}
\end{align*}
とすれば、通常の線形モデルの学習と同様に学習が可能であるが、
基底関数たち$\phi_k$と$d'$をデータに応じてうまく選ぶ必要がある。このような方法はパラメトリックな手法と呼ばれる。

利用可能なデータ数に応じて関数クラスの大きさが変わる。
具体的にはデータ数が大きくなれば関数クラスも広くなっていくような手法をノンパラメトリックな手法と呼び、
そのような手法はカーネル法を使うよりなかった。

我々の手法はカーネル法よりも効率的に学習が可能である。

さらに複雑な関係性をデータから学習して可視化することが可能である
\begin{align*}
    y = \sum_{j,k} f_{j,k} (x_j,x_k)
\end{align*}
\cite{KM01}、\cite{KM02}では二変数全変動ノルムを新しく定義し、これに基づく定式化および推定のための最適化アルゴリズムを提案した。さらに計算機実験において


\input{Matsushima/figure}

\subsubsection{組合せ線形モデルに関する研究}

組合せ線形モデルでは離散データ集合を考える。
離散データも多くの現実のデータを表現することができる。

組合せ線形モデルは回帰やクラス分類などの教師あり学習の手法のためのモデルであり、各説明変数の論理結合を特徴として線形関数を学習する手法である\cite{LMY01}。論理結合による特徴は各説明変数が同時に真である場合の効果を表すと解釈できるため、解釈性も予測性能も高い予測が可能である（図1参照）。
本研究では、論理結合による特徴を膨大な候補集合から探索する際に高速な頻出パターンマイニングのアルゴリズムを利用するなどのアプローチにより、組み合わせ線形モデルの学習アルゴリズムの改良を行う。

さらに高次対数線形モデルと呼ばれる組み合わせ線形モデルの生成モデルの研究を行った。
高次対数線形モデルは
\begin{align*}
    \log P(x;\theta) = \sum_{\phi\in B} \theta_\phi \phi(x)  - \log\left( \sum_{x'\in S} \exp\left( \sum_{\phi\in B} \theta_\phi \phi(x') \right) \right)    
\end{align*}
ここで$\phi \in B \subset \{0,1\}^n$と$\phi:\{0,1\}^n\to \{0,1\} $を同一視する。すなわち$\phi(x)$の値を以下で定義する。
\begin{align*}
    \phi(x)  = \begin{cases}
    1 & 全ての j で　\phi_j \le x_j, \\
    0 & 上記以外.
    \end{cases}
\end{align*}

\cite{LMY01}では、識別モデルにおいて教師あり学習を行う場合に効率的な学習アルゴリズムを提案した。


さらに本研究では生成モデル、すなわち二値データの分布の推定問題も対象とした。

教師なし学習の文脈では、与えられたデータの特徴を解釈可能であり、かつ真の分布をよく近似する分布を推定することを目指す。
本研究では自然スパース性（Natural Sparsity, Natural Sparseness）という概念を導入し、より自然スパース性が高いモデルを推定するような定式化と方法論を提案する。

$S\subset\{0,1\}^n$に関して真の確率分布は以下の集合$M$の元であるとする。
\begin{align*}
    M = \left\{ P(\cdot)\middle| 0<P(x)<1, \sum_{x\in S} P(x) = 1 \right\}
\end{align*}
$M$には双対平坦な座標系が存在し、それぞれの座標系での値を
自然パラメータ$\theta \in \mathbb{R}^{|S|}$
と期待パラメータ$\eta \in \mathbb{R}^{|S|}$で表す。
この時、任意の分布に関して$\eta$はスパースでない。
一方で$\theta$は非零要素を多く含むものは単純な分布とされており、
例えば単純な対数線形モデルやイジングモデルは高次の$\theta$の値を0に制限したモデルである。

さらに自然パラメータは$\theta \in \mathbb{R}^{2^n}$へ拡張できる。
この時一つの分布$P$に対しいくつかの$\theta$が対応し、スパースネスの値も表現の仕方によって変わる。（例を挙げる）
このように考えると、自然スパース性は以下のように定義することが望ましい。

\cite{HSM01}、\cite{HSM02}ではこのような意味でスパースなモデルの学習が座標降下法により効率的に学習可能であることを示した。


\subsubsection{部分空間クラスタリングに関する研究}

部分空間クラスタリングとは複数の低次元空間にデータをクラスタリングする手法である。通常のクラスタリング手法では距離的に近いデータの集まりをクラスタとみなす。一方で、部分空間クラスタリングでは同じ低次元空間にあるデータをクラスタとみなす(図~\ref{fig:my_label}を参照)。画像データや文章データなどはタスクによって部分空間クラスタリングをした方がよいということが2011年VIDAL\cite{SC}によって提案されて以降、多くの部分空間クラスタリング手法が開発されている。

\begin{figure}[h]
    \centering
    \includegraphics[width=7cm]{Matsushima/sc.eps}
    \caption{部分空間クラスタリングの3次元の例（\cite{SC}から抜粋）。距離が近い点同士ではなく、同じ平面上もしくは同じ直線上（部分空間上）にある点同士をクラスタとみなす。}
    \label{fig:my_label}
\end{figure}
\cite{NM01}は〜

\begin{thebibliography}{9}
\bibitem{F}
    Friedman, J., Hastie, T., & Tibshirani, R. (2010). 
    Applications of the lasso and grouped lasso to the estimation of sparse graphical models (pp. 1-22). Technical report, Stanford University.
\bibitem{SC}
 VIDAL, René. Subspace clustering. IEEE Signal Processing Magazine, 2011, 28.2: 52-68.
\end{thebibliography}

